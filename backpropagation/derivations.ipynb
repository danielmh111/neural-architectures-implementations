{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f8e8e0",
   "metadata": {},
   "source": [
    "# Backpropagation Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1254d",
   "metadata": {},
   "source": [
    "in this notebook i will explore backpropagation by deriving the gradients used for training nets using backpropagation. Im following along while reading the rumelhart 1986 paper \"Learning representations by back-propagating errors\" as well as some youtube videos by 3blue1brown and Andrew Ng. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab984a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sy\n",
    "from sympy import Symbol, Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e1a55",
   "metadata": {},
   "source": [
    "in a neural network, layers are often refered to as \"hidden\" if they are not the final layer or the input layer. With this in mind, the cost function for the outcome of the net depends only on the final layer. like in most learning algorithms, the point of the cost function is that we can use it to minimize the difference between the outcome of the model and the truth. if we call the outcome $\\hat{y}$ and the truth $y$, then:  $$C_0 = \\left( \\hat{y} - y \\right)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7c04a",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "we know that the outcome depends only on the activation of the final layer. we can call the activation of the final layer $a^{(L)}$, with the superscript $L$ indicating the final layer (we can say the net has $L$ layers). Theres no subscript here indicated which neuron in the layer we are describing the activation of - imagine this net has one neuron per layer for now.\n",
    "\n",
    "So then the outcome of the net is described by the activation:\n",
    "\n",
    "$$C_0 = \\left( a^{(L)} - y \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fec99c",
   "metadata": {},
   "source": [
    "we know that in a net, the activation of layer is a function of the activation of the previous layer, multiplied by weights, plus a bias.\n",
    "\n",
    "$$a^{(L)} = \\sigma \\left( w^{(L)} a^{(L - 1)} + b^{(L)} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39758b3b",
   "metadata": {},
   "source": [
    "we are using sigma here because the activation function can be a function like a sigmoid or relu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08cbf8",
   "metadata": {},
   "source": [
    "for the calculus later, we are going to give a variable name to the inside of the function:\n",
    "\n",
    "$$a^{(L)} = \\sigma \\left( z^{(L)}  \\right) $$\n",
    "where\n",
    "$$ z^{(L)} = w^{(L)} a^{(L - 1)} + b^{(L)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052615d7",
   "metadata": {},
   "source": [
    "we have only describes the activation of the final layer in terms of the next layer down - really this is also dependent on the layer before it, and so on until we get to the input vector. \n",
    "\n",
    "we can see that the cost function is functionally dependent on the input data (via the activation of layers), the weights, the biases, and the truth. treating the truth and the inputs as static parameters, we are interested in how the weights and biases affect the cost function. \n",
    "\n",
    "Firstly, look at just the weights.\n",
    "\n",
    "So we are interested in the rate of change of the cost function $C_0$ with respect to the weights $w$. Thats the partial derivitive of $C$ with respect to $w$:\n",
    "\n",
    "$$\\frac{\\partial C_0}{\\partial w^{(L)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9ef05",
   "metadata": {},
   "source": [
    "we have already described how the weights are an input at one end of a chain, and the cost function is at the other end of that chain\n",
    "\n",
    "$$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial C_0}{\\partial a^{(L)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial z^{(L)}}{\\partial w^{(L)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301e52b",
   "metadata": {},
   "source": [
    "we will do the same for the biases, this is going to look almost exactly the same\n",
    "\n",
    "$$\\frac{\\partial C_0}{\\partial b^{(L)}} = \\frac{\\partial C_0}{\\partial a^{(L)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial z^{(L)}}{\\partial b^{(L)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad8214e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining symbols for the variable in the above expressions\n",
    "\n",
    "w = Symbol(\"w\") #weights\n",
    "b = Symbol(\"b\") #biases\n",
    "\n",
    "y = Symbol(\"y\") #truth\n",
    "a_prev = Symbol(\"a_prev\") # activation of previous layer. we are going to treat this as static for now\n",
    "\n",
    "sigma = Function(\"sigma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb9e0760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a_{prev} w + b$"
      ],
      "text/plain": [
       "a_prev*w + b"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can build the intermediate fuctions z and a (activation of final layer) from these\n",
    "# type: ignore\n",
    "\n",
    "z = w * a_prev + b  \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1214f3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\sigma{\\left(a_{prev} w + b \\right)}$"
      ],
      "text/plain": [
       "sigma(a_prev*w + b)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type: ignore\n",
    "a = sigma(z)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26edf2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(- y + \\sigma{\\left(a_{prev} w + b \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "(-y + sigma(a_prev*w + b))**2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = (a - y)**2\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b49f8d",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial C_0}{\\partial a^{(L)}}\\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\\frac{\\partial z^{(L)}}{\\partial w^{(L)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf1bc9",
   "metadata": {},
   "source": [
    "we can substitude the expressions back into each variable to see how we can partially differentiate each one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593ae58",
   "metadata": {},
   "source": [
    "\n",
    "$$\\frac{\\partial z^{(L)}}{\\partial w^{(L)}} = \\frac{\\partial w^{(L)} a^{(L - 1)} + b^{(L)}}{\\partial w^{(L)}} = a^{(L - 1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36daf4ad",
   "metadata": {},
   "source": [
    "this one is straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf3bbe",
   "metadata": {},
   "source": [
    "\n",
    "$$\\frac{\\partial a^{(L)}}{\\partial z^{(L)}} = \\frac{\\partial \\sigma \\left( z^{(L)}  \\right)}{\\partial z^{(L)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0d11c",
   "metadata": {},
   "source": [
    "in this term, we have to differentiate the activation function. since im not defining what activation function im using in this nb, ill leave it in symbolic notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ddbf4",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C_0}{\\partial a^{(L)}} = \\frac{\\partial \\left( a^{(L)} - y \\right)^2}{\\partial a^{(L)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d57be",
   "metadata": {},
   "source": [
    "to evaluate the partial derivitive of the cost function with respect to either wieghts or biases, we would apply the chain rule "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c7bb0",
   "metadata": {},
   "source": [
    "we can use sympy to do these steps for partial derivitives of the cost function by both weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fd41b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dC_dw = sy.diff(C, w)\n",
    "dC_db = sy.diff(C, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd00b38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 \\left(- y + \\sigma{\\left(a_{prev} w + b \\right)}\\right) \\left. \\frac{d}{d \\xi_{1}} \\sigma{\\left(\\xi_{1} \\right)} \\right|_{\\substack{ \\xi_{1}=a_{prev} w + b }}$"
      ],
      "text/plain": [
       "2*(-y + sigma(a_prev*w + b))*Subs(Derivative(sigma(_xi_1), _xi_1), _xi_1, a_prev*w + b)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "018d6552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 a_{prev} \\left(- y + \\sigma{\\left(a_{prev} w + b \\right)}\\right) \\left. \\frac{d}{d \\xi_{1}} \\sigma{\\left(\\xi_{1} \\right)} \\right|_{\\substack{ \\xi_{1}=a_{prev} w + b }}$"
      ],
      "text/plain": [
       "2*a_prev*(-y + sigma(a_prev*w + b))*Subs(Derivative(sigma(_xi_1), _xi_1), _xi_1, a_prev*w + b)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dC_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ea8e8",
   "metadata": {},
   "source": [
    "lgtm :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6319b8",
   "metadata": {},
   "source": [
    "so far, we have described how backpropagation would work if:\n",
    "\n",
    "- we only look one layer back from the output layer\n",
    "- each layer has only one neuron\n",
    "- we only have one training example\n",
    "\n",
    "lets try and generalize to nets without this assumptions using intuative explainations\n",
    "\n",
    "we only look one layer back in the network to the activation of the previous layer, but we know that this layer's activation depends on its weights and biases, and the activation of the layer before it. So we can see that the expressions are identical for this layer, but with different indexes for each layer (i.e. $a^{(L-1)}$, $a^{(L-2)}$, $a^{(L-3)}$). this explains why we call it backpropagation - we keep going backwards through the network, adding in the effects of the weights and balances at each step. Each time we go back a layer and substitute out a_previous in the differential equation, we are going to add two more terms to the chain of partial differentials. \n",
    "\n",
    "we only looked at one neuron, and there are no subscripts given to any of the variables. for a more complex net, we can give a subscript to each neuron to keep track of it. each partial derivitive of the cost function (at each index) is then a componant of the overall cost function. A vector of all of these componants describes the gradient of a surface. gradient decent is used to step down this gradient to a minimum point - this is the neural network learning. \n",
    "\n",
    "we only have one training example - in the original expressions we give the cost function the subscript 0 to denote that it is specific to a single observation. The actual cost function is going to be an average across all training examples. in practice, we can user stochastic gradient decent to use a subset of observations at each step. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
